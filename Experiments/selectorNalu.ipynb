{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nac import NAC\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from data_generator_helper import generate_synthetic_selection_dataset\n",
    "\n",
    "#from models.nac import NeuralAccumulatorCell\n",
    "from models.nalu import NeuralArithmeticLogicUnitCell\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportLoss(loss, epoch):\n",
    "    print(\"epoch {},  \\t loss {}\".format(epoch, loss))\n",
    "    \n",
    "def train(model, optimizer, x_train, y_train, epochs, batch_size):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_batch_train = x_train[batch:(batch+batch_size),:]\n",
    "            y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "\n",
    "            out = model(x_batch_train)\n",
    "\n",
    "            loss = F.mse_loss(out, y_batch_train)\n",
    "            \n",
    "            if loss != loss:\n",
    "                break\n",
    "                print(\"nan detected\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if loss != loss:\n",
    "            break\n",
    "        \n",
    "        if epoch % 100 == 0: \n",
    "            pass\n",
    "            #reportLoss(loss.data, epoch)\n",
    "            \n",
    "    return test(model,x_train,y_train)\n",
    "        \n",
    "def test(model, x_test, y_test):\n",
    "    \n",
    "    model.eval()\n",
    "    output_test = model(x_test)\n",
    "    loss = F.mse_loss(output_test, y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1.0), (-1.0, 1.0)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eebeaf1c7407436b9f1663f4ba42097b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range (0, 1.0)\n",
      "\tTest range (0, 1.0)\n",
      "\t \tExp 0  Train loss:  2.926751729859234e-09 \t Test loss 7988809728.0\n",
      "\t \tExp 1  Train loss:  12.86323356628418 \t Test loss 139.9550323486328\n",
      "\tTest range (-1.0, 1.0)\n",
      "\t \tExp 0  Train loss:  32.0213737487793 \t Test loss 379.19134521484375\n",
      "\t \tExp 1  Train loss:  3.371293022524924e-09 \t Test loss inf\n",
      "\tTest range (0, 100)\n",
      "\t \tExp 0  Train loss:  12.39040756225586 \t Test loss inf\n",
      "\t \tExp 1  Train loss:  12.916696548461914 \t Test loss 173.10366821289062\n",
      "\tTest range (-100, 100)\n",
      "\t \tExp 0  Train loss:  2.77052640914917 \t Test loss 1.688619073999944e+32\n",
      "\t \tExp 1  Train loss:  58.18092346191406 \t Test loss 1004.892333984375\n",
      "Train range (-1.0, 1.0)\n",
      "\tTest range (0, 1.0)\n",
      "\t \tExp 0  Train loss:  9.103372278218558e-09 \t Test loss inf\n",
      "\t \tExp 1  Train loss:  1.183763842504959e-08 \t Test loss 1.9404849710060536e+19\n",
      "\tTest range (-1.0, 1.0)\n",
      "\t \tExp 0  Train loss:  7.623495612563147e-09 \t Test loss 4389760139264.0\n",
      "\t \tExp 1  Train loss:  6.278896069744633e-09 \t Test loss inf\n",
      "\tTest range (0, 100)\n",
      "\t \tExp 0  Train loss:  4.2100607444695015e-09 \t Test loss 0.7655463814735413\n",
      "\t \tExp 1  Train loss:  1.0002167094569359e-08 \t Test loss 9.934689539572367e+17\n",
      "\tTest range (-100, 100)\n",
      "\t \tExp 0  Train loss:  1.5309604251001474e-08 \t Test loss inf\n",
      "\t \tExp 1  Train loss:  8.874875057074405e-09 \t Test loss 12047.0703125\n"
     ]
    }
   ],
   "source": [
    "test_per_range = 2\n",
    "sample_size = 100\n",
    "set_size = 10000\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 1\n",
    "out_dim = 2\n",
    "num_layers = 1\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "#values = np.linspace(1,1000,10)\n",
    "values = [(0,1.0),(-1.0,1.0)]\n",
    "test_values = [(0,1.0),(-1.0,1.0),(0,100),(-100,100)]\n",
    "train_loss = np.zeros((len(values),len(test_values),test_per_range))\n",
    "test_loss = np.zeros((len(values),len(test_values),test_per_range))\n",
    "print(values)\n",
    "for j, value in tqdm(enumerate(values)):\n",
    "    \n",
    "    print(\"Train range\", value)\n",
    "    \n",
    "    for k, test_value in enumerate(test_values):\n",
    "\n",
    "        print(\"\\tTest range\", test_value)\n",
    "        min_value = value[0]\n",
    "        max_value = value[1]\n",
    "\n",
    "        test_min_value = -10\n",
    "        test_max_value = 10\n",
    "\n",
    "        train_acc = 0\n",
    "        test_acc = 0\n",
    "        for i in range(test_per_range):\n",
    "\n",
    "            model = NeuralArithmeticLogicUnitCell(in_dim, out_dim)\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "            x_train, y_train, boundaries = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                                        sample_size, set_size, boundaries = None)\n",
    "\n",
    "            x_test, y_test, _ = generate_synthetic_selection_dataset(test_min_value, test_max_value,\n",
    "                                                                        sample_size, 1, boundaries = boundaries)\n",
    "\n",
    "            train_loss[j,k,i] = train(model, optimizer, x_train, y_train, epochs, batch_size)\n",
    "\n",
    "            test_loss[j,k,i]  = test(model, x_test, y_test)\n",
    "\n",
    "            print(\"\\t \\tExp\", i, \" Train loss: \", train_loss[j,k,i], \"\\t Test loss\", test_loss[j,k,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3853574531700008e-08\n",
      "1.9732325351365785e-12\n",
      "2.520233212882678e-06\n",
      "2.1282146007719363e-07\n",
      "2.0609418029654535e-06\n"
     ]
    }
   ],
   "source": [
    "test_per_range = 1\n",
    "sample_size = 100\n",
    "set_size = 100\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 1\n",
    "out_dim = 2\n",
    "num_layers = 1\n",
    "\n",
    "#values = np.linspace(1,1000,10)\n",
    "values = [[0.0,1.0], \n",
    "          [-0.001,0.001],\n",
    "          [0,100],\n",
    "          [-100.0,100.0],\n",
    "          [-1000.0,1000.0]]\n",
    "\n",
    "for value in values:\n",
    "    min_value, max_value=  value\n",
    "\n",
    "    x_test, y_test, _ = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                                sample_size, set_size, boundaries =boundaries)\n",
    "\n",
    "    #x_test = x_test.type(torch.DoubleTensor)\n",
    "    #y_test = y_test.type(torch.DoubleTensor)\n",
    "    test_loss  = test(model, x_test, y_test)\n",
    "\n",
    "    print(float(test_loss.data.numpy())/torch.max(x_test).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_loss_exp4.csv\", train_loss, delimiter=',', fmt='%2.2f')\n",
    "np.savetxt(\"test_loss_exp3.csv\", test_loss, delimiter=',', fmt='%2.2f')\n",
    "\n",
    "np.savetxt(\"all_train_loss_exp3.csv\", np.mean(train_loss<0.001,axis=1), delimiter=',', fmt='%2.2f')\n",
    "np.savetxt(\"all_test_loss_exp3.csv\", np.mean(test_loss<0.001,axis=1), delimiter=',', fmt='%2.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00,  1.4762e-08]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-0a6a8f03ce92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgumbel_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "from activationFunctions.Gumbel import gumbel_softmax\n",
    "x = np.linspace(-10,10, 100)\n",
    "taus = [0.05,0.5,0.8,1]\n",
    "results = np.zeros((len(taus),len(x)))\n",
    "for i, tau in enumerate(taus):\n",
    "    for j,val in enumerate(x):\n",
    "        val = val* torch.ones(1,2)\n",
    "\n",
    "        y = gumbel_softmax(val, tau,)\n",
    "        print(y)\n",
    "        results[i, j] = y\n",
    "\n",
    "    plt.plot(x, results[i, :])\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
