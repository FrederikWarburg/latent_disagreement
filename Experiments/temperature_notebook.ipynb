{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models_new.nac import NAC\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from data_generator_helper import generate_synthetic_selection_dataset\n",
    "\n",
    "from models_new.nac import NeuralAccumulatorCell\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportLoss(loss, epoch):\n",
    "    print(\"epoch {},  \\t loss {}\".format(epoch, loss))\n",
    "    \n",
    "def train(model, optimizer, x_train, y_train, epochs, batch_size):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        #print('G is: ',model.G.data)\n",
    "        #print('g is: ',torch.sigmoid(model.G))\n",
    "        \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_batch_train = x_train[batch:(batch+batch_size),:]\n",
    "            y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "\n",
    "            out = model(x_batch_train)\n",
    "\n",
    "            loss = F.mse_loss(out, y_batch_train)\n",
    "            \n",
    "            if loss != loss:\n",
    "                break\n",
    "                print(\"nan detected\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if loss != loss:\n",
    "            break\n",
    "        \n",
    "        if epoch % 250 == 0: \n",
    "            #pass\n",
    "            reportLoss(loss.data, epoch)\n",
    "            \n",
    "        \n",
    "        model.temperature = epoch * 0.001\n",
    "            \n",
    "    return test(model,x_train,y_train)\n",
    "        \n",
    "def test(model, x_test, y_test):\n",
    "    \n",
    "    model.eval()\n",
    "    output_test = model(x_test)\n",
    "    loss = F.mse_loss(output_test, y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "\n",
    "class NeuralAccumulatorCell(nn.Module):\n",
    "    \"\"\"A Neural Accumulator (NAC) cell [1].\n",
    "    Attributes:\n",
    "        in_dim: size of the input sample.\n",
    "        out_dim: size of the output sample.\n",
    "    Sources:\n",
    "        [1]: https://arxiv.org/abs/1808.00508\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, ini):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.initial = ini\n",
    "        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.temperature = 0.005\n",
    "\n",
    "        self.register_parameter('W_hat', self.W_hat)\n",
    "        self.register_parameter('M_hat', self.M_hat)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        self._reset_params(ini)\n",
    "\n",
    "    def _reset_params(self,ini):\n",
    "        if ini =='Kai_uni':\n",
    "            init.kaiming_uniform_(self.W_hat)\n",
    "            init.kaiming_uniform_(self.M_hat)\n",
    "\n",
    "        if ini =='Xav_norm':\n",
    "            init.xavier_normal_(self.W_hat)\n",
    "            init.xavier_normal_(self.M_hat)\n",
    "\n",
    "        if ini =='Kai_norm':\n",
    "            init.kaiming_normal_(self.W_hat)\n",
    "            init.kaiming_normal_(self.M_hat)\n",
    "\n",
    "        if ini =='Zeros':\n",
    "            init.zeros_(self.W_hat)\n",
    "            init.zeros_(self.M_hat)\n",
    "\n",
    "        if ini =='Ones':\n",
    "            init.ones_(self.W_hat)\n",
    "            init.ones_(self.M_hat)\n",
    "\n",
    "    def forward(self, input):\n",
    "        W = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "        return F.linear(input, W, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_dim={}, out_dim={}'.format(\n",
    "            self.in_dim, self.out_dim\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "\n",
    "class NeuralArithmeticLogicUnitCell(nn.Module):\n",
    "    \"\"\"A Neural Arithmetic Logic Unit (NALU) cell [1].\n",
    "\n",
    "    Attributes:\n",
    "        in_dim: size of the input sample.\n",
    "        out_dim: size of the output sample.\n",
    "\n",
    "    Sources:\n",
    "        [1]: https://arxiv.org/abs/1808.00508\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, ini):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.G = Parameter(torch.Tensor(self.out_dim,self.in_dim))\n",
    "        #print(self.G)\n",
    "        self.nac = NeuralAccumulatorCell(in_dim, out_dim, ini)\n",
    "        self.register_parameter('bias', None)\n",
    "        self.temperature = 0.005\n",
    "        \n",
    "        if ini =='Kai_uni':\n",
    "            init.kaiming_uniform_(self.G, a=math.sqrt(5))\n",
    "\n",
    "        if ini =='Xav_norm':\n",
    "            init.xavier_normal_(self.G)\n",
    "\n",
    "        if ini =='Kai_norm':\n",
    "            init.kaiming_normal_(self.G)\n",
    "\n",
    "        if ini =='Zeros':\n",
    "            init.zeros_(self.G)\n",
    "\n",
    "        if ini =='Ones':\n",
    "            init.ones_(self.G)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        a = self.nac(self.temperature*input)\n",
    "\n",
    "        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
    "        #g = torch.sigmoid(self.G)\n",
    "        #print(g)\n",
    "        add_sub = g * a\n",
    "        log_input = torch.log(torch.abs(input) + self.eps)\n",
    "        m = torch.exp(self.nac(log_input))\n",
    "        mul_div = (1 - g) * m\n",
    "        y = add_sub + mul_div\n",
    "        return y\n",
    "    \n",
    "    def extra_repr(self):\n",
    "        return 'in_dim={}, out_dim={}'.format(\n",
    "            self.in_dim, self.out_dim\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40309da40d54a85be7d90ff51b5df71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing range:  [0, 1]\n",
      "epoch 0,  \t loss 116.44195556640625\n",
      "epoch 250,  \t loss 0.2151702642440796\n",
      "epoch 500,  \t loss 0.13974036276340485\n",
      "epoch 750,  \t loss 0.650138795375824\n",
      "epoch 0,  \t loss 17.60400390625\n",
      "epoch 250,  \t loss 0.07584310322999954\n",
      "epoch 500,  \t loss 0.0015391649212688208\n",
      "epoch 750,  \t loss 0.12700334191322327\n",
      "epoch 0,  \t loss 40.7957878112793\n",
      "epoch 250,  \t loss 2.7070817947387695\n",
      "epoch 500,  \t loss 0.950836181640625\n",
      "epoch 750,  \t loss 0.06971385329961777\n",
      "epoch 0,  \t loss 182.17532348632812\n",
      "epoch 250,  \t loss 0.035194896161556244\n",
      "epoch 500,  \t loss 0.37300893664360046\n",
      "epoch 750,  \t loss 0.37376341223716736\n",
      "epoch 0,  \t loss 2.183091878890991\n",
      "epoch 250,  \t loss 0.030947769060730934\n",
      "epoch 500,  \t loss 0.023782700300216675\n",
      "epoch 750,  \t loss 0.002782810013741255\n",
      "Testing range:  [0, 10]\n",
      "epoch 0,  \t loss 345.4822692871094\n",
      "epoch 250,  \t loss 3.1837666034698486\n",
      "epoch 500,  \t loss 1.8403294086456299\n",
      "epoch 750,  \t loss 1.6760534048080444\n",
      "epoch 0,  \t loss 350.3602294921875\n",
      "epoch 250,  \t loss 2.9235992431640625\n",
      "epoch 500,  \t loss 0.055262595415115356\n",
      "epoch 750,  \t loss 0.0026391763240098953\n",
      "epoch 0,  \t loss 22.991106033325195\n",
      "epoch 250,  \t loss 2.124624252319336\n",
      "epoch 500,  \t loss 0.9453297853469849\n",
      "epoch 750,  \t loss 0.49576807022094727\n",
      "epoch 0,  \t loss 61.61827087402344\n",
      "epoch 250,  \t loss 0.15348757803440094\n",
      "epoch 500,  \t loss 0.25813454389572144\n",
      "epoch 750,  \t loss 0.22096338868141174\n",
      "epoch 0,  \t loss 70.23776245117188\n",
      "epoch 250,  \t loss 0.08703095465898514\n",
      "epoch 500,  \t loss 0.17957647144794464\n",
      "epoch 750,  \t loss 0.2375192940235138\n",
      "Testing range:  [0, 100]\n",
      "epoch 0,  \t loss 178.53567504882812\n",
      "epoch 250,  \t loss 0.2823355793952942\n",
      "epoch 500,  \t loss 0.3159073293209076\n",
      "epoch 750,  \t loss 0.09153706580400467\n",
      "epoch 0,  \t loss 405.9780578613281\n",
      "epoch 250,  \t loss 5.647611618041992\n",
      "epoch 500,  \t loss 0.5833687782287598\n",
      "epoch 750,  \t loss 0.08737362176179886\n",
      "epoch 0,  \t loss 530.734130859375\n",
      "epoch 250,  \t loss 1.2445576190948486\n",
      "epoch 500,  \t loss 7.296124458312988\n",
      "epoch 750,  \t loss 2.5243868827819824\n",
      "epoch 0,  \t loss 349.97216796875\n",
      "epoch 250,  \t loss 0.378666490316391\n",
      "epoch 500,  \t loss 0.5486169457435608\n",
      "epoch 750,  \t loss 0.6447665095329285\n",
      "epoch 0,  \t loss 109.07176971435547\n",
      "epoch 250,  \t loss 0.061185650527477264\n",
      "epoch 500,  \t loss 2.5656867027282715\n",
      "epoch 750,  \t loss 0.09769682586193085\n",
      "Testing range:  [-10, 10]\n",
      "epoch 0,  \t loss 207.02066040039062\n",
      "epoch 250,  \t loss 1.8147326707839966\n",
      "epoch 500,  \t loss 0.7334343194961548\n",
      "epoch 750,  \t loss 3.1518054008483887\n",
      "epoch 0,  \t loss 81.69375610351562\n",
      "epoch 250,  \t loss 0.6212599873542786\n",
      "epoch 500,  \t loss 0.20227211713790894\n",
      "epoch 750,  \t loss 0.6359085440635681\n",
      "epoch 0,  \t loss 0.376970112323761\n",
      "epoch 250,  \t loss 0.0001074700485332869\n",
      "epoch 500,  \t loss 2.115221468557138e-05\n",
      "epoch 750,  \t loss 0.00022074852313380688\n",
      "epoch 0,  \t loss 111.52676391601562\n",
      "epoch 250,  \t loss 0.06398170441389084\n",
      "epoch 500,  \t loss 0.7716102004051208\n",
      "epoch 750,  \t loss 0.006301654037088156\n",
      "epoch 0,  \t loss 110.70442199707031\n",
      "epoch 250,  \t loss 4.405951023101807\n",
      "epoch 500,  \t loss 4.016541481018066\n",
      "epoch 750,  \t loss 2.7883052825927734\n",
      "Testing range:  [-100, 100]\n",
      "epoch 0,  \t loss 80.86470794677734\n",
      "epoch 250,  \t loss 0.3791031241416931\n",
      "epoch 500,  \t loss 0.09162888675928116\n",
      "epoch 750,  \t loss 0.15820544958114624\n",
      "epoch 0,  \t loss 11.373761177062988\n",
      "epoch 250,  \t loss 0.13466644287109375\n",
      "epoch 500,  \t loss 0.027070317417383194\n",
      "epoch 750,  \t loss 0.028706230223178864\n",
      "epoch 0,  \t loss 36.51759719848633\n",
      "epoch 250,  \t loss 1.5576194524765015\n",
      "epoch 500,  \t loss 1.4777910709381104\n",
      "epoch 750,  \t loss 0.8519700169563293\n",
      "epoch 0,  \t loss 107.06436157226562\n",
      "epoch 250,  \t loss 0.24520058929920197\n",
      "epoch 500,  \t loss 0.059011466801166534\n",
      "epoch 750,  \t loss 0.7355899214744568\n",
      "epoch 0,  \t loss 8.48071575164795\n",
      "epoch 250,  \t loss 0.7925046682357788\n",
      "epoch 500,  \t loss 0.6104538440704346\n",
      "epoch 750,  \t loss 0.5409612655639648\n",
      "Testing range:  [-1000, 1000]\n",
      "epoch 0,  \t loss 86.51471710205078\n",
      "epoch 250,  \t loss 0.17722931504249573\n",
      "epoch 500,  \t loss 0.7756175994873047\n",
      "epoch 750,  \t loss 0.009046617895364761\n",
      "epoch 0,  \t loss 214.06686401367188\n",
      "epoch 250,  \t loss 2.029118537902832\n",
      "epoch 500,  \t loss 47.433929443359375\n",
      "epoch 750,  \t loss 15.889142990112305\n",
      "epoch 0,  \t loss 341.396728515625\n",
      "epoch 250,  \t loss 30.381080627441406\n",
      "epoch 500,  \t loss 2.3833820819854736\n",
      "epoch 750,  \t loss 7.4062724113464355\n",
      "epoch 0,  \t loss 127.60604095458984\n",
      "epoch 250,  \t loss 0.2532272934913635\n",
      "epoch 500,  \t loss 0.009447921067476273\n",
      "epoch 750,  \t loss 0.005870858207345009\n",
      "epoch 0,  \t loss 17.413476943969727\n",
      "epoch 250,  \t loss 0.04020600765943527\n",
      "epoch 500,  \t loss 0.0033142392057925463\n",
      "epoch 750,  \t loss 0.05826728045940399\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_per_range = 5\n",
    "sample_size = 100\n",
    "set_size = 100\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 1\n",
    "out_dim = 2\n",
    "num_layers = 1\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "\n",
    "#values = np.linspace(1,1000,10)\n",
    "#inits = ['Kai_norm']\n",
    "values = [[0,1],[0,10],[0,100],[-10,10],[-100,100],[-1000,1000]]\n",
    "\n",
    "train_loss = np.zeros((len(values),test_per_range))\n",
    "test_loss = np.zeros((len(values),test_per_range))\n",
    "\n",
    "for j, val in tqdm(enumerate(values)):\n",
    "    \n",
    "    print(\"Testing range: \",val)\n",
    "    \n",
    "    test_min_val = val[0]\n",
    "    test_max_val = val[1]\n",
    "    \n",
    "    min_value = 0\n",
    "    max_value = 1\n",
    "    \n",
    "    for i in range(test_per_range):\n",
    "\n",
    "        model = NeuralArithmeticLogicUnitCell(in_dim, out_dim, 'Kai_norm')\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "        x_train, y_train, boundaries = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                                    sample_size, set_size, boundaries = None)\n",
    "\n",
    "        x_test, y_test, _ = generate_synthetic_selection_dataset(test_min_val, test_max_val,\n",
    "                                                                    sample_size, set_size, boundaries = boundaries)\n",
    "\n",
    "        train_loss[j, i] = train(model, optimizer, x_train, y_train, epochs, batch_size)\n",
    "\n",
    "        test_loss[j, i]  = test(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"train_temperature_w.csv\", np.mean(train_loss,axis=1), delimiter=',', fmt='%2.2f')\n",
    "np.savetxt(\"test_temperature_w.csv\", np.mean(test_loss,axis=1), delimiter=',', fmt='%2.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.76521545e+02, 2.88264980e+01, 1.13808670e+02, 2.21286774e+02,\n",
       "        2.72706532e+00],\n",
       "       [3.67789844e+04, 3.12936777e+04, 4.61900098e+03, 6.97806787e+03,\n",
       "        1.41748994e+04],\n",
       "       [2.93838450e+06, 3.30380475e+06, 5.31210300e+06, 5.46541150e+06,\n",
       "        3.06754225e+06],\n",
       "       [1.06228271e+03, 7.25237915e+02, 4.13608627e+01, 6.53541565e+02,\n",
       "        9.14548035e+02],\n",
       "       [9.50787266e+04, 3.10303691e+04, 3.73622109e+04, 7.64114609e+04,\n",
       "        4.02363555e+04],\n",
       "       [1.27414340e+07, 9.74038100e+06, 1.34249410e+07, 7.07834100e+06,\n",
       "        2.27169300e+06]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.63411049842836\n",
      "18768.92607421875\n",
      "4017449.2\n",
      "679.3942184448242\n",
      "56023.824609375\n",
      "9051358.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(test_loss)):\n",
    "    print(np.mean(test_loss[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
