{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from data_generator_helper import generate_synthetic_selection_dataset\n",
    "from models_new.nalu import NALU\n",
    "from models_new.nac import NAC\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D as plt3\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from ipywidgets import widgets\n",
    "\n",
    "\n",
    "from models.mlp import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportLoss(loss):\n",
    "    print(loss)\n",
    "    \n",
    "def train_mlp(model, optimizer, x_train, y_train, epochs, batch_size, model_param):\n",
    "    losses = np.zeros((epochs,len(x_train)//batch_size))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_batch_train = x_train[batch:(batch+batch_size),:]\n",
    "            y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "            out = model(x_batch_train)\n",
    "\n",
    "            loss = F.mse_loss(out, y_batch_train)\n",
    "            \n",
    "            if loss != loss:\n",
    "                print(\"nan detected\")\n",
    "            losses[epoch,batch] = loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(loss)\n",
    "               \n",
    "    return test_mlp(model,x_train,y_train),losses\n",
    "\n",
    "     \n",
    "    \n",
    "def test_mlp(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    output_test = model(x_test)\n",
    "    loss = F.mse_loss(output_test, y_test)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670a2af1090c43d78b98d433ed0b6de6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nntest_NALU_['NALU']_test_0_2_50_1000 0.004288416355848312\n",
      "nntest_NALU_['NALU']_test_1_2_50_1000 0.10299801826477051\n",
      "nntest_NALU_['NALU']_test_2_2_50_1000 0.06978246569633484\n",
      "nntest_NALU_['NALU']_test_3_2_50_1000 0.0033828439190983772\n",
      "nntest_NALU_['NALU']_test_4_2_50_1000 0.04298591986298561\n",
      "nntest_NALU_['NALU']_test_5_2_50_1000 0.006954716052860022\n",
      "nntest_NALU_['NALU']_test_6_2_50_1000 0.01935393549501896\n",
      "nntest_NALU_['NALU']_test_7_2_50_1000 0.01746761053800583\n",
      "nntest_NALU_['NALU']_test_8_2_50_1000 0.022643879055976868\n",
      "nntest_NALU_['NALU']_test_9_2_50_1000 0.2977808713912964\n"
     ]
    }
   ],
   "source": [
    "ini = ['Kai_uni','Xav_norm','Kai_norm','Zeros','Ones']\n",
    "model_param = \"NALU\"\n",
    "\n",
    "net_shape = [[1,100],[2,50],[2,100],[3,50],[3,100]]\n",
    "net_shape = [[2,50]]\n",
    "ini = [[\"NALU\"]]\n",
    "#ini = ini[0]\n",
    "for shp in net_shape:\n",
    "    for init in ini:\n",
    "        test_per_range = 1\n",
    "        sample_size = 100\n",
    "        set_size = 100000\n",
    "\n",
    "        in_dim = sample_size\n",
    "        hidden_dim = shp[1]\n",
    "        out_dim = 2\n",
    "        num_layers = shp[0]\n",
    "\n",
    "        lr = 0.01\n",
    "        epochs = 1000\n",
    "        batch_size = 1\n",
    "        values = [1] #np.linspace(1,1000,10)\n",
    "        losses = np.zeros((epochs, set_size//batch_size))\n",
    "        train_loss = [] #np.zeros((len(values),10))\n",
    "        test_loss = [] # np.zeros((len(values),10))\n",
    "\n",
    "\n",
    "        for j, value in tqdm(enumerate(values)):\n",
    "            model = MLP(num_layers, in_dim, hidden_dim, out_dim, ini)\n",
    "            optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "            min_value = 0#-value\n",
    "            max_value = value\n",
    "\n",
    "            train_acc = 0\n",
    "            test_acc = 0\n",
    "            for k in range(test_per_range):\n",
    "                i = 0\n",
    "\n",
    "                x_train, y_train, boundaries = generate_synthetic_selection_dataset(min_value, \n",
    "                                                                                    max_value, sample_size, \n",
    "                                                                                    set_size, boundaries = None)\n",
    "\n",
    "                x_test, y_test, _ = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                                 sample_size, set_size, boundaries = boundaries)\n",
    "                #bounds[j][i] = boundaries\n",
    "\n",
    "\n",
    "                train_loss,losses = train_mlp(model, optimizer, x_train, y_train, epochs, batch_size, model_param)\n",
    "                test_loss  = test_mlp(model, x_test, y_test)\n",
    "\n",
    "                filename = \"nntest_\" + model_param +\"_\"+ str(init) +\"_\"+ \"test_\" + str(k) +\"_\"+ str(num_layers) + \"_\" + str(hidden_dim) + \"_\" + str(set_size) \n",
    "                print(filename,losses[-1,-1])\n",
    "                \n",
    "                t = [[param for param in children.parameters()] for children in model.model.children()]\n",
    "                #for children in model.model.children():\n",
    "                #    print(\"Children:\")\n",
    "                #    print(children)\n",
    "                #    plist = [param for param in children.parameters()]\n",
    "                #    print(\"Plist:\")\n",
    "                #    print(plist)\n",
    "                np.save(filename,(losses[-1,-1],t,boundaries))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0050, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0007, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0000, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f3f11c579ea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_loss\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest_mlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-7c801945e51a>\u001b[0m in \u001b[0;36mtrain_mlp\u001b[0;34m(model, optimizer, x_train, y_train, epochs, batch_size, model_param)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "init = ['Kai_uni']\n",
    "shp = [3,50]\n",
    "\n",
    "\n",
    "test_per_range = 1\n",
    "sample_size = 100\n",
    "set_size = 100000\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = shp[1]\n",
    "out_dim = 2\n",
    "num_layers = shp[0]\n",
    "\n",
    "lr = 0.001\n",
    "epochs = 100\n",
    "batch_size = 1\n",
    "losses = np.zeros((epochs, set_size//batch_size))\n",
    "train_loss = [] #np.zeros((len(values),10))\n",
    "test_loss = [] # np.zeros((len(values),10))\n",
    "\n",
    "model = MLP(num_layers, in_dim, hidden_dim, out_dim, ini)\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "\n",
    "min_value = 0#-value\n",
    "max_value = 1\n",
    "\n",
    "x_train, y_train, boundaries = generate_synthetic_selection_dataset(min_value, \n",
    "                                                                    max_value, sample_size, \n",
    "                                                                    set_size, boundaries = None)\n",
    "\n",
    "x_test, y_test, _ = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                 sample_size, set_size, boundaries = boundaries)\n",
    "\n",
    "\n",
    "train_loss,losses = train_mlp(model, optimizer, x_train, y_train, epochs, batch_size, model_param)\n",
    "test_loss  = test_mlp(model, x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0003001852\n",
      "586.4125\n",
      "0.03669976\n",
      "363.1091\n",
      "3636.069\n"
     ]
    }
   ],
   "source": [
    "valu = [[0,1],[-0.001,0.001],[0,100],[-100,100],[-1000,1000]]\n",
    "for val in valu:\n",
    "\n",
    "    min_value = val[0]#-value\n",
    "    max_value = val[1]\n",
    "\n",
    "    x_test, y_test, _ = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                     sample_size, set_size, boundaries = boundaries)\n",
    "\n",
    "    test_loss  = test_mlp(model, x_test, y_test)\n",
    "    print(test_loss.data.numpy() / np.max(x_test.data.numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
