{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nac import NAC\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from data_generator_helper import generate_synthetic_selection_dataset_with_function\n",
    "\n",
    "from models.nac import NAC\n",
    "from models.nalu import NALU\n",
    "from models.mlp import MLP\n",
    "import torch\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "from torchvision import datasets\n",
    "from tensorboardX import SummaryWriter\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportLoss(loss, epoch):\n",
    "    pass\n",
    "    #print(\"epoch {},  \\t loss {}\".format(epoch, loss))\n",
    "    \n",
    "def train(model, optimizer, x_train, y_train, epochs, batch_size):\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "    \n",
    "        \n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_batch_train = x_train[batch:(batch+batch_size),:]\n",
    "            y_batch_train = y_train[batch:(batch+batch_size)].unsqueeze(1)\n",
    "\n",
    "            out = model(x_batch_train)\n",
    "\n",
    "            loss = F.mse_loss(out, y_batch_train)\n",
    "            \n",
    "            if loss != loss:\n",
    "                break\n",
    "                print(\"nan detected\")\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if loss != loss:\n",
    "            break\n",
    "        \n",
    "        if epoch % 10 == 0: \n",
    "            #pass\n",
    "            reportLoss(loss.data, epoch)\n",
    "            \n",
    "    return test(model,x_train,y_train)\n",
    "        \n",
    "def test(model, x_test, y_test):\n",
    "    \n",
    "    model.eval()\n",
    "    output_test = model(x_test)\n",
    "    #print(np.shape(output_test), np.shape(y_test))\n",
    "    loss = F.mse_loss(output_test.squeeze(), y_test)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "substraction\n",
      "multiplication\n",
      "division\n",
      "[[4.85954166e-04 2.59443914e-05 1.87909770e+00 3.82747118e-07\n",
      "  5.18940568e-01 3.20285410e-01 6.95403742e-07 2.40179986e-07\n",
      "             nan 2.46495162e-08]\n",
      " [3.40786064e-05 2.43923583e-04 2.13930740e+01 4.79689837e-02\n",
      "  1.65488973e-06 9.71723011e-07 3.76453003e+02 3.35519530e-07\n",
      "  3.34467227e-03 1.49235717e-08]\n",
      " [           nan            nan 2.29801154e+00 6.54596558e+01\n",
      "             nan 2.37650989e+22 7.69081402e+00 7.83073716e-03\n",
      "  4.05623823e-01 2.30565857e+02]\n",
      " [2.66282063e-04 3.17145459e-05 8.11696082e-05 3.49985040e-03\n",
      "  1.42790714e-05 1.95699075e-04 3.27088775e+26 6.13776501e-04\n",
      "  4.76807418e-06 6.20079152e-02]] [[1.03390468e-02 1.81317902e+01 1.19780798e+01 6.47572142e-06\n",
      "  6.69842291e+00 6.04221439e+00 2.09628251e-06 4.79015716e-06\n",
      "  4.90428972e+00 1.76628876e+00]\n",
      " [7.88017758e-04 5.70063433e-03 6.98404395e+03 1.23117816e+00\n",
      "  1.86514571e-05 3.63139297e-05 3.71942780e+02 1.76205640e-06\n",
      "  7.25337029e+00 4.75359315e-07]\n",
      " [           nan            nan 4.62237892e+01 6.47323633e+03\n",
      "             nan 9.30755354e+32 2.61595490e+02 3.22305173e-01\n",
      "  1.05103360e+08 5.88357617e+03]\n",
      " [3.79540469e-03 2.43987795e-03 6.47071376e-02 8.29677135e-02\n",
      "  5.06925862e-04 4.14270302e-03 6.82536192e+18 7.74587551e-03\n",
      "  9.79984397e-05 2.74017181e+01]]\n"
     ]
    }
   ],
   "source": [
    "test_per_range = 10\n",
    "sample_size = 100\n",
    "set_size = 100\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 2\n",
    "out_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "\n",
    "#values = np.linspace(1,1000,10)\n",
    "min_value = 0\n",
    "max_value = 1\n",
    "\n",
    "\n",
    "functions = ['add', 'substraction','multiplication','division']\n",
    "\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "train_loss = np.zeros((len(functions),10))\n",
    "test_loss = np.zeros((len(functions),10))\n",
    "\n",
    "for j, function in enumerate(functions):\n",
    "    print(function)\n",
    "    for i in range(test_per_range):\n",
    "\n",
    "        model = NALU(2,in_dim,2,out_dim)\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "        x_train, y_train, boundaries = generate_synthetic_selection_dataset_with_function(min_value, max_value,\n",
    "                                                                    sample_size, set_size, function, boundaries = None)\n",
    "\n",
    "        x_test, y_test, _ = generate_synthetic_selection_dataset_with_function(min_value, max_value,\n",
    "                                                                    sample_size, set_size, function, boundaries = boundaries)\n",
    "\n",
    "        train_loss[j, i] = train(model, optimizer, x_train, y_train, epochs, batch_size)\n",
    "\n",
    "        test_loss[j, i]  = test(model, x_test, y_test)\n",
    "\n",
    "print(train_loss, test_loss)\n",
    "\n",
    "np.save('interpolation_train_nac.npy', train_loss)\n",
    "np.save('interpolation_test_nac.npy', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAC\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-39b05dd55a33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'interpolation_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtabulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtablefmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"latex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloatfmt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".2f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NALU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tabulate.py\u001b[0m in \u001b[0;36mtabulate\u001b[0;34m(tabular_data, headers, tablefmt, floatfmt, numalign, stralign, missingval, showindex, disable_numparse)\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0mtabular_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1246\u001b[0m     list_of_lists, headers = _normalize_tabular_data(\n\u001b[0;32m-> 1247\u001b[0;31m             tabular_data, headers, showindex=showindex)\n\u001b[0m\u001b[1;32m   1248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[0;31m# empty values in the first column of RST tables should be escaped (issue #82)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/tabulate.py\u001b[0m in \u001b[0;36m_normalize_tabular_data\u001b[0;34m(tabular_data, headers, showindex)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_text_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m     \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m     \u001b[0;31m# add or remove an index column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not iterable"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "print('NAC')\n",
    "print()\n",
    "train_loss = np.load('interpolation_train.npy')\n",
    "\n",
    "print(tabulate(, tablefmt=\"latex\", floatfmt=\".2f\"))\n",
    "print()\n",
    "print('NALU')\n",
    "print()\n",
    "train_loss = np.load('interpolation_train_nalu.npy')\n",
    "print(tabulate(train_loss, tablefmt=\"latex\", floatfmt=\".2f\"))\n",
    "print()\n",
    "print('MLP')\n",
    "print()\n",
    "train_loss = np.load('interpolation_train_mlp.npy')\n",
    "print(tabulate(train_loss, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.86868692e-07, 3.09087117e-10, 1.25332308e+00, 4.51949376e-08,\n",
       "        8.33877202e-05, 1.24205178e-06, 1.97263479e+00, 1.26257819e-05,\n",
       "        1.00000000e+04, 7.22626166e-04],\n",
       "       [3.11444783e-05, 2.12043952e-02, 3.31377851e-05, 8.23099708e-06,\n",
       "        1.68213560e-07, 1.07135618e+00, 6.89761706e-08, 5.66886946e-08,\n",
       "        1.44564140e-07, 3.08401342e-02],\n",
       "       [8.03858280e-01, 3.29356492e-02, 9.70888443e+01, 7.33434379e-01,\n",
       "        1.13834694e+02, 1.10023651e+02, 1.91587734e+00, 1.39648020e-01,\n",
       "        2.45329595e+00, 6.26496300e-02],\n",
       "       [4.09411034e-04, 5.42274751e-02, 1.93151991e-05, 4.45608096e-03,\n",
       "        3.71856877e-05, 8.55812734e+04, 5.22784321e-05, 2.25649637e-05,\n",
       "        2.86052132e+00, 3.27280373e-04]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "\\hline\n",
      "     0.00 &    0.42 &  0.25 \\\\\n",
      "     0.00 &    0.11 &  0.19 \\\\\n",
      "  4193.47 &   32.71 &  4.39 \\\\\n",
      " 57820.80 & 8558.42 & 14.39 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "train_loss1 = np.load('interpolation_train.npy')\n",
    "train_loss2 = np.load('interpolation_train_nalu.npy')\n",
    "train_loss3 = np.load('interpolation_train_mlp.npy')\n",
    "train_loss2[np.isnan(train_loss2)] =1\n",
    "tab = np.array([np.mean(train_loss1,axis=1),np.mean(train_loss2,axis=1),np.mean(train_loss3,axis=1)]).T\n",
    "print(tabulate(tab, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAC\n",
      "\n",
      "\\begin{tabular}{rrrrrrrrrr}\n",
      "\\hline\n",
      "                     0.08 &                     0.41 &                    0.08 &                    0.12 &                    0.07 &                     0.12 &                    0.66 &                   0.62 &                      0.26 &                    1.19 \\\\\n",
      "                     0.48 &                     0.22 &                    0.65 &                    0.61 &                    0.32 &                   657.69 &                    0.30 &                   0.50 &                   1741.00 &                    1.72 \\\\\n",
      " 109446627677939171328.00 & 434699856117147631616.00 & 10486715295215910912.00 & 29150320445318234112.00 & 11665469825517355008.00 & 817360618540029181952.00 & 34548229845372895232.00 & 1798006763329421312.00 & 1337661131646421696512.00 & 27602291636293337088.00 \\\\\n",
      "              46450008.00 &           10467905536.00 &            184399056.00 &             18638976.00 &             55738892.00 &            6116191232.00 &                68128.56 &             9168513.00 &             4795484160.00 &           1950368768.00 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "NALU\n",
      "\n",
      "\\begin{tabular}{rrrrrrrrrr}\n",
      "\\hline\n",
      " nan &      nan    &       inf    &                  nan    & 17303232512.00 & 75641167872.00 &  nan    &               nan    &          nan    &         11329690624.00 \\\\\n",
      " nan &      nan    &       nan    &                  inf    &         nan    &         nan    &  nan    &               nan    & 389031854080.00 &                7160.39 \\\\\n",
      " nan &      nan    &       nan    & 14532738865036263424.00 &         nan    &         nan    &  nan    & 52491058772180992.00 &          nan    & 3117952742849511424.00 \\\\\n",
      " nan & 13245120.00 & 382973824.00 &                  nan    &         nan    &         111.76 & 9282.19 &               nan    &          nan    &              171836.56 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n",
      "\n",
      "MLP\n",
      "\n",
      "\\begin{tabular}{rrrrrrrrrr}\n",
      "\\hline\n",
      "           5385819648.00 &          1831046400.00 &            4282549760.00 &            2187393024.00 &             852701760.00 &           3256731136.00 &          2076186880.00 &           2651827456.00 &           1904165376.00 &            2407508480.00 \\\\\n",
      "           1235050240.00 &           459825120.00 &              44188200.00 &            4349298176.00 &              86403768.00 &           1955437824.00 &          2289455872.00 &           2798802688.00 &             74751904.00 &             165428784.00 \\\\\n",
      " 16785369909512110080.00 & 8309248514882273280.00 & 193544832814153728000.00 & 156054054766154612736.00 & 267747952269872594944.00 & 98554399300259938304.00 & 2316471988119404544.00 & 16859578148293967872.00 & 13033235902191632384.00 & 598742874409792438272.00 \\\\\n",
      "              7384160.50 &        701494460416.00 &             471390400.00 &                   689.13 &            3842987008.00 &                   20.17 &             2526514.50 &            721948480.00 &           2147137280.00 &                   184.02 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "print('NAC')\n",
    "print()\n",
    "train_loss = np.load('extrapolation_test.npy')\n",
    "print(tabulate(train_loss[:,:,-1], tablefmt=\"latex\", floatfmt=\".2f\"))\n",
    "print()\n",
    "print('NALU')\n",
    "print()\n",
    "train_loss = np.load('extrapolation_test_nalu.npy')\n",
    "print(tabulate(train_loss[:,:,-1], tablefmt=\"latex\", floatfmt=\".2f\"))\n",
    "print()\n",
    "print('MLP')\n",
    "print()\n",
    "train_loss = np.load('extrapolation_test_mlp.npy')\n",
    "print(tabulate(train_loss[:,:,-1], tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrr}\n",
      "\\hline\n",
      "                     0.36 &                 inf    &            2683592992.00 \\\\\n",
      "                   240.35 &                 inf    &            1345864257.60 \\\\\n",
      " 281441926779258470400.00 & 1770318266665795584.00 & 137194801802323263488.00 \\\\\n",
      "            2364441326.96 &            39640017.95 &           70868783515.23 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "train_loss1 = np.load('extrapolation_test.npy')\n",
    "train_loss2 = np.load('extrapolation_test_nalu.npy')\n",
    "train_loss3 = np.load('extrapolation_test_mlp.npy')\n",
    "train_loss2[np.isnan(train_loss2)] =1\n",
    "tab = np.array([np.mean(train_loss1[:,:,-1],axis=1),np.mean(train_loss2[:,:,-1],axis=1),np.mean(train_loss3[:,:,-1],axis=1)]).T\n",
    "print(tabulate(tab, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "substraction\n",
      "multiplication\n",
      "division\n",
      "[[7.65911639e-02 1.34952331e-03 2.81305944e-07            inf\n",
      "  2.88584033e-05 1.41003346e+00 1.15786518e-08 1.29876249e-02\n",
      "  8.84207487e-01 4.62090759e+01]\n",
      " [3.25179749e+01 2.00051836e-05 1.42350439e-06 1.37535210e-06\n",
      "  3.51581216e-01 1.02534448e-03 2.28039490e-08 1.41416326e-01\n",
      "  2.80396052e-04 3.72918301e-08]\n",
      " [1.92051560e-01 1.70244062e+00 1.29957933e+01 2.41013408e+00\n",
      "             nan            nan            nan 7.24028625e+01\n",
      "  1.84814392e+02            nan]\n",
      " [1.27205695e-03 4.63084543e-05 1.14710722e-03 7.46485428e-04\n",
      "  2.11485195e+00 6.69707544e-03 1.01572706e-03 1.06183634e-05\n",
      "  4.94413252e-04 1.32017249e-05]] [[[1.62720664e+04            nan            nan            nan]\n",
      "  [1.22660422e+00            nan            nan            nan]\n",
      "  [7.40672700e+06            nan            nan            nan]\n",
      "  [1.47837842e+03 1.28802633e+05 1.32930960e+07 1.34853952e+09]\n",
      "  [7.89500698e-02 4.18284180e+02            nan            nan]\n",
      "  [5.64692188e+04            nan            nan            nan]\n",
      "  [7.85114316e-05 3.11639812e-02 6.79504156e+00 1.20206677e+03]\n",
      "  [3.38211456e+02            nan            nan            nan]\n",
      "  [7.68139844e+04            nan            nan            nan]\n",
      "  [4.43445663e+01 3.79814150e+06 3.22459743e+12 8.18500170e+20]]\n",
      "\n",
      " [[4.65888969e+05            inf            nan            nan]\n",
      "  [2.26835441e-03 2.18422934e-01 2.71673489e+01 2.49256958e+03]\n",
      "  [3.68760425e+06 4.61373809e+14            nan            nan]\n",
      "  [4.59157848e+00 3.17205719e+05 2.79528796e+10 1.04649197e+16]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [3.32805127e-01 5.91495934e+01            nan            nan]\n",
      "  [8.56536190e-06 1.29149423e-03 1.13462798e-01 1.33471193e+01]\n",
      "  [1.08214912e+04            inf            nan            nan]\n",
      "  [3.31642777e-02 3.61078835e+00 4.11765625e+02            nan]\n",
      "  [1.13017113e-05            nan            nan            nan]]\n",
      "\n",
      " [[2.42336075e+06 2.85309829e+10 2.97177478e+14            nan]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [7.36615040e+07 7.13829188e+11 6.97666059e+15 6.81758386e+19]\n",
      "  [2.07753080e+07 1.87472675e+11 1.88926203e+15 1.89390702e+19]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [           nan            nan            nan            nan]\n",
      "  [4.85260812e+05 6.80907878e+09 1.06243557e+14 1.63644961e+18]]\n",
      "\n",
      " [[9.69368439e+01 2.61247729e+03 5.51931953e+04 1.05999838e+06]\n",
      "  [1.10433613e+13 1.43965583e+35            inf            inf]\n",
      "  [6.03827286e+00 1.06726538e+03 6.28406406e+04 1.57615400e+07]\n",
      "  [1.59066530e+14 2.67785100e+24            inf            inf]\n",
      "  [3.20835449e+02 2.56625671e+02 1.22994214e+03            nan]\n",
      "  [6.23414307e+02 7.36300156e+04 7.71374800e+06 7.69350400e+08]\n",
      "  [9.47263412e+01 2.00857764e+03 3.36937461e+04 4.56701750e+05]\n",
      "  [7.28860023e+31            inf            nan            nan]\n",
      "  [6.01853561e+00 5.74357071e+01 3.93752411e+02            nan]\n",
      "  [5.00735535e+02 2.61908583e-03 2.51754932e-03 2.46252096e-03]]]\n"
     ]
    }
   ],
   "source": [
    "test_per_range = 10\n",
    "sample_size = 100\n",
    "set_size = 100\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 2\n",
    "out_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "lr = 0.01\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "\n",
    "#values = np.linspace(1,1000,10)\n",
    "min_value = 0\n",
    "max_value = 1\n",
    "\n",
    "functions = ['add', 'substraction','multiplication','division']\n",
    "\n",
    "train_acc = 0\n",
    "test_acc = 0\n",
    "\n",
    "train_loss = np.zeros((len(functions),10))\n",
    "test_loss = np.zeros((len(functions),10, 4 ))\n",
    "\n",
    "for j, function in enumerate(functions):\n",
    "    print(function)\n",
    "    for i in range(test_per_range):\n",
    "\n",
    "        model = NALU(2,in_dim,2,out_dim)\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "        x_train, y_train, boundaries = generate_synthetic_selection_dataset_with_function(min_value, max_value,\n",
    "                                                                    sample_size, set_size, function, boundaries = None)\n",
    "        \n",
    "        train_loss[j, i] = train(model, optimizer, x_train, y_train, epochs, batch_size)\n",
    "\n",
    "        for z, k in enumerate(np.logspace(1,4,4)):\n",
    "            x_test, y_test, _ = generate_synthetic_selection_dataset_with_function(min_value, k,\n",
    "                                            sample_size, set_size, function, boundaries = boundaries)\n",
    "        \n",
    "            test_loss[j, i, z]  = test(model, x_test, y_test)\n",
    "\n",
    "print(train_loss, test_loss)\n",
    "np.save('extrapolation_train_nac.npy', train_loss)\n",
    "np.save('extrapolation_test_nac.npy', test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = np.load('sample_extrapolation_test_nalu.npy')\n",
    "train_loss = np.load('sample_interpolation_train_nalu.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{rrrr}\n",
      "\\hline\n",
      "     6158324.00 &      2723805.50 &                       2141833.47 & 0.00 \\\\\n",
      "      504677.62 &      3087595.00 &                       1462754.00 & 0.00 \\\\\n",
      " 40464441344.00 & 746878861312.00 &                  259035480064.00 & 0.00 \\\\\n",
      "          17.17 &           42.09 & 24235527161657027285965864960.00 & 0.00 \\\\\n",
      "\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "tab = np.nanmedian(test_loss,axis=2)\n",
    "print(tabulate(tab, tablefmt=\"latex\", floatfmt=\".2f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
