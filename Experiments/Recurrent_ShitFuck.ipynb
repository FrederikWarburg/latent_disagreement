{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#plt.style.use('seaborn')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_arithmetic_dataset(arithmetic_op, min_value, max_value, sample_size, set_size, boundaries = None):\n",
    "    \"\"\"\n",
    "    generates a dataset of integers for the synthetics arithmetic task\n",
    "\n",
    "    :param arithmetic_op: the type of operation to perform on the sum of the two sub sections can be either :\n",
    "    [\"add\" , \"subtract\", \"multiply\", \"divide\", \"root\", \"square\"]\n",
    "    :param min_value: the minimum possible value of the generated integers\n",
    "    :param max_value: the maximum possible value of the generated integers\n",
    "    :param sample_size: the number of integers per sample\n",
    "    :param set_size: the number of samples in the dataset\n",
    "    :param boundaries: [Optional] an iterable of 4 integer indices in the following format :\n",
    "    [start of 1st section, end of 1st section, start of 2nd section, end of 2nd section]\n",
    "    if None, the boundaries are randomly generated.\n",
    "    :return: the training dataset input, the training true outputs, the boundaries of the sub sections used\n",
    "    \"\"\"\n",
    "    scaled_input_values = np.random.uniform(min_value, max_value, (set_size, sample_size))\n",
    "\n",
    "    if boundaries is None:\n",
    "        boundaries = [np.random.randint(sample_size) for i in range(4)]\n",
    "        boundaries[1] = np.random.randint(boundaries[0], sample_size)\n",
    "        boundaries[3] = np.random.randint(boundaries[2], sample_size)\n",
    "    else:\n",
    "        if len(boundaries) != 4:\n",
    "            raise ValueError(\"boundaries is expected to be a list of 4 elements but found {}\".format(len(boundaries)))\n",
    "\n",
    "    a = np.array([np.sum(sample[boundaries[0]:boundaries[1]]) for sample in scaled_input_values])\n",
    "    b = np.array([np.sum(sample[boundaries[2]:boundaries[3]]) for sample in scaled_input_values])\n",
    "    \n",
    "    true_outputs = None\n",
    "    if \"add\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a + b\n",
    "    elif \"sub\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a - b\n",
    "    elif \"mult\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a * b\n",
    "    elif \"div\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a / b\n",
    "    elif \"square\" == str.lower(arithmetic_op):\n",
    "        true_outputs = a * a\n",
    "    elif \"root\" in str.lower(arithmetic_op):\n",
    "        true_outputs = np.sqrt(a)\n",
    "    \n",
    "    scaled_input_values = torch.tensor(scaled_input_values, dtype=torch.float32)\n",
    "    true_outputs = torch.tensor(true_outputs, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    return scaled_input_values, true_outputs, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recurrent_synthetic_arithmetic_dataset(arithmetic_op, min_value, max_value, sample_size, sequence_size, set_size, boundaries = None):\n",
    "    \"\"\"\n",
    "    generates a dataset of integers for the synthetics arithmetic task\n",
    "\n",
    "    :param arithmetic_op: the type of operation to perform on the sum of the two sub sections can be either :\n",
    "    [\"add\" , \"subtract\", \"multiply\", \"divide\", \"root\", \"square\"]\n",
    "    :param min_value: the minimum possible value of the generated integers\n",
    "    :param max_value: the maximum possible value of the generated integers\n",
    "    :param sample_size: the number of integers per sample\n",
    "    :param set_size: the number of samples in the dataset\n",
    "    :param boundaries: [Optional] an iterable of 4 integer indices in the following format :\n",
    "    [start of 1st section, end of 1st section, start of 2nd section, end of 2nd section]\n",
    "    if None, the boundaries are randomly generated.\n",
    "    :return: the training dataset input, the training true outputs, the boundaries of the sub sections used\n",
    "    \"\"\"\n",
    "    scaled_input_values = np.random.uniform(min_value, max_value, (set_size, sample_size, sequence_size))\n",
    "\n",
    "    if boundaries is None:\n",
    "        boundaries = [np.random.randint(sample_size) for i in range(4)]\n",
    "        boundaries[1] = np.random.randint(boundaries[0], sample_size)\n",
    "        boundaries[3] = np.random.randint(boundaries[2], sample_size)\n",
    "    else:\n",
    "        if len(boundaries) != 4:\n",
    "            raise ValueError(\"boundaries is expected to be a list of 4 elements but found {}\".format(len(boundaries)))\n",
    "\n",
    "    a = np.array([np.sum(sample[boundaries[0]:boundaries[1],:]) for sample in scaled_input_values])\n",
    "    b = np.array([np.sum(sample[boundaries[2]:boundaries[3],:]) for sample in scaled_input_values])\n",
    "    \n",
    "    true_outputs = None\n",
    "    if \"add\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a + b\n",
    "    elif \"sub\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a - b\n",
    "    elif \"mult\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a * b\n",
    "    elif \"div\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a / b\n",
    "    elif \"square\" == str.lower(arithmetic_op):\n",
    "        true_outputs = a * a\n",
    "    elif \"root\" in str.lower(arithmetic_op):\n",
    "        true_outputs = np.sqrt(a)\n",
    "    \n",
    "    scaled_input_values = torch.tensor(scaled_input_values, dtype=torch.float32)\n",
    "    true_outputs = torch.tensor(true_outputs, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    return scaled_input_values, true_outputs, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 50, 10]), torch.Size([10, 1]), [42, 44, 21, 22])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arithmetic_op  ='add'\n",
    "min_value = 0\n",
    "max_value = 2\n",
    "sample_size = 50\n",
    "sequence_size = 10\n",
    "set_size = 10\n",
    "\n",
    "# X = (set_size, sample_size, sequence_size)\n",
    "\n",
    "# set_size: how many input data\n",
    "# (sample_size, sequence_size) the size of each input data\n",
    "\n",
    "X, y, boundaries = generate_recurrent_synthetic_arithmetic_dataset(\n",
    "                                                arithmetic_op, min_value,\n",
    "                                                max_value, sample_size,\n",
    "                                                sequence_size, set_size)\n",
    "\n",
    "\n",
    "np.shape(X),np.shape(y), boundaries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralAccumulatorCell(nn.Module):\n",
    "    \"\"\"A Neural Accumulator (NAC) cell [1].\n",
    "\n",
    "    Attributes:\n",
    "        in_dim: size of the input sample.\n",
    "        out_dim: size of the output sample.\n",
    "\n",
    "    Sources:\n",
    "        [1]: https://arxiv.org/abs/1808.00508\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.W_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.M_hat = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "\n",
    "        self.register_parameter('W_hat', self.W_hat)\n",
    "        self.register_parameter('M_hat', self.M_hat)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        self._reset_params()\n",
    "\n",
    "    def _reset_params(self):\n",
    "        init.kaiming_uniform_(self.W_hat)\n",
    "        init.kaiming_uniform_(self.M_hat)\n",
    "\n",
    "    def forward(self, input):\n",
    "        W = torch.tanh(self.W_hat) * torch.sigmoid(self.M_hat)\n",
    "        return F.linear(input, W, self.bias)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_dim={}, out_dim={}'.format(\n",
    "            self.in_dim, self.out_dim\n",
    "        )\n",
    "\n",
    "\n",
    "class NAC(nn.Module):\n",
    "    \"\"\"A stack of NAC layers.\n",
    "\n",
    "    Attributes:\n",
    "        num_layers: the number of NAC layers.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(\n",
    "                NeuralAccumulatorCell(\n",
    "                    hidden_dim if i > 0 else in_dim,\n",
    "                    hidden_dim if i < num_layers - 1 else out_dim,\n",
    "                )\n",
    "            )\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralArithmeticLogicUnitCell(nn.Module):\n",
    "    \"\"\"A Neural Arithmetic Logic Unit (NALU) cell [1].\n",
    "\n",
    "    Attributes:\n",
    "        in_dim: size of the input sample.\n",
    "        out_dim: size of the output sample.\n",
    "\n",
    "    Sources:\n",
    "        [1]: https://arxiv.org/abs/1808.00508\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(NeuralArithmeticLogicUnitCell).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.eps = 1e-10\n",
    "\n",
    "        self.G = Parameter(torch.Tensor(out_dim, in_dim))\n",
    "        self.nac = NeuralAccumulatorCell(in_dim, out_dim)\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "        init.kaiming_uniform_(self.G,a=np.sqrt(5))\n",
    "\n",
    "    def forward(self, input):\n",
    "        a = self.nac(input)\n",
    "        g = torch.sigmoid(F.linear(input, self.G, self.bias))\n",
    "        add_sub = g * a\n",
    "        log_input = torch.log(torch.abs(input) + self.eps)\n",
    "        m = torch.exp(self.nac(log_input))\n",
    "        mul_div = (1 - g) * m\n",
    "        y = add_sub + mul_div\n",
    "        return y\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_dim={}, out_dim={}'.format(\n",
    "            self.in_dim, self.out_dim\n",
    "        )\n",
    "\n",
    "\n",
    "class NALU(nn.Module):\n",
    "    \"\"\"A stack of NAC layers.\n",
    "    Attributes:\n",
    "        num_layers: the number of NAC layers.\n",
    "        in_dim: the size of the input sample.\n",
    "        hidden_dim: the size of the hidden layers.\n",
    "        out_dim: the size of the output.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_layers, in_dim, hidden_dim, out_dim):\n",
    "        super(NALU).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.in_dim = in_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            layers.append(\n",
    "                NeuralArithmeticLogicUnitCell(\n",
    "                    hidden_dim if i > 0 else in_dim,\n",
    "                    hidden_dim if i < num_layers - 1 else out_dim,\n",
    "                )\n",
    "            )\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNALU(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RNALU, self).__init__()\n",
    "        self.NALU_1 = NeuralArithmeticLogicUnitCell(input_dim, output_dim)\n",
    "        self.NALU_2 = NeuralArithmeticLogicUnitCell(output_dim*2, output_dim)\n",
    "        \n",
    "    def forward(self, y0, x1):\n",
    "        self.y1 = self.NALU_2(torch.cat([y0, self.NALU_1(x1)], dim=1))\n",
    "        return y0, self.y1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(RNALU, self).__init__()\n",
    "        self.W = Parameter(init.kaiming_uniform_(torch.Tensor(output_dim, output_dim)))\n",
    "        self.V = Parameter(init.kaiming_uniform_(torch.Tensor(output_dim, output_dim)))\n",
    "        self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, x0, x1):\n",
    "        self.y0 = nn.tanh(F.linear(x0, self.W, self.bias))\n",
    "        self.y1 = nn.tanh(F.linear(self.y0, self.V) + F.linear(x1, self.W, self.bias) )\n",
    "        return self.y0, self.y1                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NALUNet(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(NALUNet, self).__init__()\n",
    "        self.net = RNALU(2, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, 1)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.prev = torch.zeros([x.size(0), self.hidden_dim], dtype=torch.float)              \n",
    "        for i in range(x.size(1)):\n",
    "            self.y0, self.y1 = self.net(self.prev ,x[:,i,:])\n",
    "            self.prev = self.y1\n",
    "        out = self.lin(self.y1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \n",
    "    ARITHMETIC_FUNCTIONS = {\n",
    "        'add': lambda x, y: x + y,\n",
    "        'sub': lambda x, y: x - y,\n",
    "        'mul': lambda x, y: x * y,\n",
    "        'div': lambda x, y: x / y,\n",
    "        'squared': lambda x, y: torch.pow(x, 2),\n",
    "        'root': lambda x, y: torch.sqrt(x),\n",
    "    }\n",
    "\n",
    "    def generate_data_recurrent(num_train, num_test, time_series_len, fn, support):\n",
    "        a = torch.FloatTensor(num_train + num_test, time_series_len).uniform_(*support).unsqueeze_(1)\n",
    "        b = torch.FloatTensor(num_train + num_test, time_series_len).uniform_(*support).unsqueeze_(1)\n",
    "\n",
    "        X = torch.FloatTensor(num_train + num_test, time_series_len, 2)\n",
    "        y = torch.FloatTensor(num_train + num_test, 1)\n",
    "        \n",
    "        for i in range(num_train + num_test):\n",
    "            X[i,:,0] = a[i,:]\n",
    "            X[i,:,1] = b[i,:]\n",
    "            y[i,0] = fn(torch.sum(a[i,:]), torch.sum(b[i,:]))\n",
    "            \n",
    "        X = torch.FloatTensor(X)\n",
    "        y = torch.FloatTensor(y).unsqueeze_(1)\n",
    "        indices = list(range(num_train + num_test))\n",
    "        np.random.shuffle(indices)\n",
    "        X_train, y_train = X[indices[num_test:]], y[indices[num_test:]]\n",
    "        X_test, y_test = X[indices[:num_test]], y[indices[:num_test]]\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([550, 1, 10])\n",
      "torch.Size([50, 1, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([500, 10, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = DataGenerator.ARITHMETIC_FUNCTIONS['add']\n",
    "RANGE_Inter = [1, 10]\n",
    "RANGE_Extra = [1, 100]\n",
    "\n",
    "#X_train, y_train, X_valid, y_valid = dg.generate_data(num_train=500, num_test=50, dim=100, num_sum=5, fn=fn, support=RANGE_Inter)\n",
    "#_, _, X_test, y_test = dg.generate_data(num_train=0, num_test=50, dim=100, num_sum=5, fn=fn, support=RANGE_Extra)\n",
    "\n",
    "X_train, y_train, X_valid, y_valid = DataGenerator.generate_data_recurrent(num_train=500, num_test=50, time_series_len = 10, fn=fn, support=RANGE_Inter)\n",
    "_, _, X_test, y_test = DataGenerator.generate_data_recurrent(num_train=0, num_test=50, time_series_len = 10, fn=fn, support=RANGE_Extra)\n",
    "\n",
    "\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[5.9296, 4.6421],\n",
      "         [8.0179, 1.0727]],\n",
      "\n",
      "        [[1.1866, 4.2403],\n",
      "         [7.5139, 6.7523]]])\n",
      "tensor([[[19.6621]],\n",
      "\n",
      "        [[19.6930]]])\n",
      "tensor(19.6930)\n"
     ]
    }
   ],
   "source": [
    "fn = DataGenerator.ARITHMETIC_FUNCTIONS['add']\n",
    "RANGE_Inter = [1, 10]\n",
    "RANGE_Extra = [1, 100]\n",
    "\n",
    "#X_train, y_train, X_valid, y_valid = dg.generate_data(num_train=500, num_test=50, dim=100, num_sum=5, fn=fn, support=RANGE_Inter)\n",
    "#_, _, X_test, y_test = dg.generate_data(num_train=0, num_test=50, dim=100, num_sum=5, fn=fn, support=RANGE_Extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "cannot assign parameters before Module.__init__() call",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-9d479a7cd4da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Has to be NAC/NALU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNALUNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#Defining criterion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-107-abba4188c336>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_dim)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNALUNet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNALU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-779f99a89f9e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, output_dim)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNALU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNALU_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralArithmeticLogicUnitCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNALU_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralArithmeticLogicUnitCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-f65967c2c82e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, out_dim)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralAccumulatorCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 raise AttributeError(\n\u001b[0;32m--> 530\u001b[0;31m                     \"cannot assign parameters before Module.__init__() call\")\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mremove_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: cannot assign parameters before Module.__init__() call"
     ]
    }
   ],
   "source": [
    "# Defining the network to train on\n",
    "\n",
    "## Has to be NAC/NALU\n",
    "net = NALUNet(1)\n",
    "\n",
    "#Defining criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Defining optimizers\n",
    "optimizer = optim.RMSprop(net_1.parameters(), lr = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting hyperparameters and gettings epoch sizes\n",
    "num_epochs = int(1e4)\n",
    "num_samples_train = X_train.shape[0]\n",
    "num_samples_valid = X_valid.shape[0]\n",
    "num_samples_test = X_test.shape[0]\n",
    "\n",
    "# setting up lists for handling loss/accuracy\n",
    "train_loss = [[], []]\n",
    "valid_loss = [[], []]\n",
    "test_loss = [[], []]\n",
    "\n",
    "NAC_W1_a, NAC_W1_b = [], []\n",
    "NAC_W2_a, NAC_W2_b = [], []\n",
    "\n",
    "NALU_g1, NALU_g2 = [], []\n",
    "NALU_g1_var, NALU_g2_var = [], []\n",
    "\n",
    "Reg_W1_a, Reg_W1_b = [], []\n",
    "Reg_W2_a, Reg_W2_b = [], []\n",
    "\n",
    "get_slice = lambda i, size: range(i * size, (i + 1) * size)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward -> Backprob -> Update params\n",
    "    \n",
    "    ## Training Network 1\n",
    "    net_1.train()\n",
    "    output_1= net_1(X_train)\n",
    "    loss_1 = criterion(output_1, y_train[:,:,0])\n",
    "    optimizer_1.zero_grad()\n",
    "    loss_1.backward()\n",
    "    optimizer_1.step()\n",
    "    \n",
    "    ## Training Network 2\n",
    "    net_2.train()\n",
    "    output_2 = net_2(X_train)\n",
    "    loss_2 = criterion(output_2[:,-1,:], y_train[:,:,0])\n",
    "    optimizer_2.zero_grad()\n",
    "    loss_2.backward()\n",
    "    optimizer_2.step()\n",
    " \n",
    "    # Evaluating\n",
    "    \n",
    "    net_1.eval()\n",
    "    net_2.eval()\n",
    "    \n",
    "    ### Evaluate training\n",
    "\n",
    "    train_preds_1 = net_1(X_train).data.numpy()\n",
    "    train_preds_2 = net_2(X_train)[:,-1,:].data.numpy()\n",
    "    train_targs = y_train.data.numpy()[:,:,0]\n",
    "    \n",
    "    ### Evaluate validation\n",
    "\n",
    "    val_preds_1 = net_1(X_valid).data.numpy()\n",
    "    val_preds_2 = net_2(X_valid)[:,-1,:].data.numpy()\n",
    "    val_targs = y_valid.data.numpy()[:,:,0]\n",
    "        \n",
    "#     NALU_g1.append(np.mean(net_1.g1.data.numpy(),axis=0).tolist())\n",
    "#     NALU_g2.append(np.mean(net_1.g2.data.numpy(),axis=0).tolist())\n",
    "    \n",
    "#     NALU_g1_var.append(np.std(net_1.g1.data.numpy(),axis=0).tolist())\n",
    "#     NALU_g2_var.append(np.std(net_1.g2.data.numpy(),axis=0).tolist())  \n",
    "    \n",
    "    ### Evaluate test (outside of range)\n",
    "   \n",
    "    test_preds_1 = net_1(X_test).data.numpy()\n",
    "    test_preds_2 = net_2(X_test)[:,-1,:].data.numpy()\n",
    "    test_targs = y_test.data.numpy()[:,:,0]\n",
    "\n",
    "    train_loss_cur_1 = np.mean(np.abs(train_preds_1 - train_targs))\n",
    "    train_loss_cur_2 = np.mean(np.abs(train_preds_2 - train_targs))\n",
    "    valid_loss_cur_1 = np.mean(np.abs(val_preds_1 - val_targs))\n",
    "    valid_loss_cur_2 = np.mean(np.abs(val_preds_2 - val_targs))\n",
    "    test_loss_cur_1 = np.mean(np.abs(test_preds_1 - test_targs))\n",
    "    test_loss_cur_2 = np.mean(np.abs(test_preds_2 - test_targs))\n",
    "    \n",
    "    train_loss[0].append(train_loss_cur_1)\n",
    "    train_loss[1].append(train_loss_cur_2)\n",
    "    valid_loss[0].append(valid_loss_cur_1)\n",
    "    valid_loss[1].append(valid_loss_cur_2)\n",
    "    test_loss[0].append(test_loss_cur_1)\n",
    "    test_loss[1].append(test_loss_cur_2)\n",
    "    \n",
    "    #Getting the weights\n",
    "    #NAC_W1_a.append(net_1.W1[:,0].data.numpy().tolist())\n",
    "    #NAC_W1_b.append(net_1.W1[:,1].data.numpy().tolist())\n",
    "    #NAC_W2_a.append(net_1.W2[:,0].data.numpy().tolist())\n",
    "    #NAC_W2_b.append(net_1.W2[:,1].data.numpy().tolist())\n",
    "    \n",
    "    #Reg_W1_a.append(net_2.W1[:,0].data.numpy().tolist())\n",
    "    #Reg_W1_b.append(net_2.W1[:,1].data.numpy().tolist())\n",
    "    #Reg_W2_a.append(net_2.W2[:,0].data.numpy().tolist())\n",
    "    #Reg_W2_b.append(net_2.W2[:,1].data.numpy().tolist())\n",
    "    \n",
    "    if epoch % (num_epochs/10) == 0:\n",
    "        print(\"Epoch %2i : Train losses (%f, %f), Test losses (%f, %f)\" % (\n",
    "                epoch+1, train_loss_cur_1, train_loss_cur_2, test_loss_cur_1, test_loss_cur_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training/Validation/Test Losses\n",
    "epoch = np.arange(len(train_loss[0]))\n",
    "f, (ax1, ax2) = plt.subplots(1,2, sharex = True, sharey = True, figsize=(15,5))\n",
    "\n",
    "ax1.plot(epoch, train_loss[0], 'r', epoch, valid_loss[0], 'b')\n",
    "ax2.plot(epoch, train_loss[1], 'r', epoch, valid_loss[1], 'b')\n",
    "\n",
    "ax1.legend(['Train Loss','Validation Loss'])\n",
    "ax2.legend(['Train Loss','Validation Loss'])\n",
    "\n",
    "ax1.set_title('NALU Net'), ax2.set_title('Reg Net')\n",
    "ax1.set_xlabel('Updates'), ax2.set_xlabel('Updates'), ax1.set_ylabel('Loss')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.plot(epoch, test_loss[0], 'r', epoch, test_loss[1], 'b')\n",
    "plt.title('Extrapolation Loss')\n",
    "plt.legend(['NALU Test Loss','Reg Test Loss'])\n",
    "plt.xlabel('Updates'), plt.ylabel('Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FEATURES_NUM = 100\n",
    "epochs = 100\n",
    "batch_size = 10\n",
    "learning_rate = 0.1\n",
    "operator = \"add\"\n",
    "\n",
    "in_dim = FEATURES_NUM\n",
    "hidden_dim = 2\n",
    "out_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "dim = in_dim # dimensition for generating data\n",
    "\n",
    "model = NALU(num_layers, in_dim, hidden_dim, out_dim)\n",
    "print(model)\n",
    "\n",
    "epochs = 2000\n",
    "#lrs = [0.001, 0.005, 0.001 ]\n",
    "#bss = [10,20,50]\n",
    "lrs = [0.05]\n",
    "bss = [1]\n",
    "i = 0\n",
    "for learning_rate in lrs:\n",
    "    for batch_size in bss:\n",
    "        print(\"lrs_\" + str(learning_rate) + \"_bss_\" + str(batch_size))\n",
    "        #print(i)\n",
    "        #i += 1\n",
    "        #filename = \"lrs_\" + str(learning_rate) + \"_bss_\" + str(batch_size) + \".txt\"\n",
    "        #file = open(filename,\"a\")\n",
    "        #boundaries = [0,10,5,14]\n",
    "        X_train, y_train, boundaries = generate_synthetic_arithmetic_dataset(operator, 100, 110, FEATURES_NUM, 1000)\n",
    "        X_test, y_test, _ = generate_synthetic_arithmetic_dataset(operator, 0, 10, FEATURES_NUM, 1000, boundaries)\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=learning_rate)\n",
    "        print(boundaries)\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            for batch in range(len(X_train) // batch_size):\n",
    "                model.train()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                X_batch_train = X_train[batch:(batch+batch_size),:]\n",
    "                y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "\n",
    "                out = model(X_batch_train)\n",
    "                if np.sum(np.isnan(out.detach().numpy())) > 0:\n",
    "                    print(epoch,batch)\n",
    "                loss = F.mse_loss(out, y_batch_train)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                model.eval()\n",
    "\n",
    "                X_batch_test = X_test[batch:(batch+batch_size),:]\n",
    "                y_batch_test = y_test[batch:(batch+batch_size),:]\n",
    "\n",
    "                output_test = model(X_batch_test)\n",
    "\n",
    "                acc = np.sum(np.isclose(output_test.detach().numpy(), y_batch_test.detach().numpy(), atol=.1, rtol=0)) / len(y_batch_test)\n",
    "                print(\"epoch \\t\", epoch, \"\\t loss \\t\",  loss.detach().numpy(), \"\\t acc \\t\", acc)\n",
    "                #file.write(\"epoch \\t\" + str(epoch) + \"\\t loss \\t\" + str(loss.detach().numpy()) + \"\\t acc \\t\" + str(acc)+ \"\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
