{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from data_generator_helper import generate_synthetic_selection_dataset\n",
    "from models_new.nalu_b import NALU\n",
    "from models_new.nac import NAC\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as vutils\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.mplot3d import Axes3D as plt3\n",
    "\n",
    "from ipywidgets import interactive\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reportLoss(loss):\n",
    "    print(loss)\n",
    "    \n",
    "def train(model, optimizer, x_train, y_train, epochs, batch_size, model_param):\n",
    "    '''\n",
    "    if model_param == \"NAC\":\n",
    "        weights = np.zeros((epochs,len(x_train)//batch_size,out_dim,sample_size,3))\n",
    "    elif model_param == \"NALU\":\n",
    "        weights = np.zeros((epochs,len(x_train)//batch_size,out_dim,sample_size,4))\n",
    "        g_prev = np.zeros((epochs,len(x_train)//batch_size,out_dim))\n",
    "    losses = np.zeros((epochs,len(x_train)//batch_size))\n",
    "    '''\n",
    "    for epoch in range(epochs):\n",
    "        for batch in range(len(x_train) // batch_size):\n",
    "            '''\n",
    "            # Save weights\n",
    "            for children in model.model.children():\n",
    "                plist = [param for param in children.parameters()]\n",
    "                if model_param == \"NAC\":\n",
    "                    W_hat = plist[0]\n",
    "                    M_hat = plist[1]\n",
    "                elif model_param == \"NALU\":\n",
    "                    W_hat = plist[1]\n",
    "                    M_hat = plist[2]\n",
    "                    weights[epoch,batch,:,:,3] = plist[0].detach().numpy()\n",
    "                W = torch.tanh(W_hat) * torch.sigmoid(M_hat)\n",
    "                weights[epoch,batch,:,:,0] = W_hat.detach().numpy()\n",
    "                weights[epoch,batch,:,:,1] = M_hat.detach().numpy()\n",
    "                weights[epoch,batch,:,:,2] = W.detach().numpy()\n",
    "            '''\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x_batch_train = x_train[batch:(batch+batch_size),:]\n",
    "            y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "            '''\n",
    "            if model_param == \"NALU\":\n",
    "                g_prev[epoch,batch,:] = F.linear(plist[0],x_batch_train,).detach().numpy().flatten()\n",
    "                #print(np.shape(g_prev),np.shape(F.linear(plist[0],x_batch_train).detach().numpy()))\n",
    "            '''\n",
    "            out = model(x_batch_train)\n",
    "            \n",
    "            loss = F.mse_loss(out, y_batch_train)\n",
    "            \n",
    "            if loss != loss:\n",
    "                print(\"nan detected\")\n",
    "            #losses[epoch,batch] = loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "           \n",
    "    return test(model,x_train,y_train)\n",
    "\n",
    "     \n",
    "    \n",
    "def test(model, x_test, y_test):\n",
    "    model.eval()\n",
    "    output_test = model(x_test)\n",
    "    loss = F.mse_loss(output_test, y_test)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13254cf11104c13ab7b034fcc170e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: [0,1]\n",
      "Interpolation Loss:  2.18e-11\n",
      "Extrapolation Loss: 3.02e+19 6.54e+20 7.91e+50 4.48e+48 8.02e+139 4.52e+140 \n",
      "Interpolation Loss:  1.88e-11\n",
      "Extrapolation Loss: 1.07e+14 5.96e+13 2.49e+36 1.22e+33 3.50e+104 3.61e+107 \n",
      "Interpolation Loss:  2.17e-11\n",
      "Extrapolation Loss: 3.94e+14 1.32e+16 1.39e+38 5.83e+38 1.23e+114 5.87e+112 \n",
      "Interpolation Loss:  8.80e-12\n",
      "Extrapolation Loss: 1.27e-03 5.53e-04 4.71e+04 6.56e+04 1.77e+30 1.24e+30 \n",
      "Interpolation Loss:  1.46e-11\n",
      "Extrapolation Loss: 4.32e-05 5.81e-04 3.92e+02 6.20e+01 3.15e+23 6.63e+22 \n",
      "Interpolation Loss:  2.43e-11\n",
      "Extrapolation Loss: 7.34e+24 5.35e+28 4.36e+63 2.88e+60 6.91e+171 6.14e+173 \n",
      "Interpolation Loss:  3.46e-11\n",
      "Extrapolation Loss: 2.81e+17 1.11e+16 1.05e+37 3.92e+40 6.22e+116 8.03e+115 \n",
      "Interpolation Loss:  1.59e-11\n",
      "Extrapolation Loss: 9.34e-01 8.95e-01 2.86e+10 1.79e+10 6.93e+42 7.78e+42 \n",
      "Interpolation Loss:  1.76e-11\n",
      "Extrapolation Loss: 8.53e+13 4.22e+11 4.85e+33 1.49e+32 1.12e+104 6.12e+103 \n",
      "Interpolation Loss:  1.22e-11\n",
      "Extrapolation Loss: 3.62e-02 1.10e-01 1.15e+07 3.08e+07 1.14e+38 2.95e+36 \n",
      "\n",
      "\n",
      "Train range: [-1,1]\n",
      "Interpolation Loss:  1.27e-11\n",
      "Extrapolation Loss: 4.26e+11 7.43e+12 2.15e+36 4.60e+36 4.35e+104 4.19e+105 \n",
      "Interpolation Loss:  6.55e-12\n",
      "Extrapolation Loss: 8.78e-04 4.06e-04 3.85e+03 1.56e+04 2.08e+29 1.44e+29 \n",
      "Interpolation Loss:  1.11e-11\n",
      "Extrapolation Loss: 1.66e+10 3.33e+13 1.03e+28 2.77e+28 3.06e+89 5.61e+87 \n",
      "Interpolation Loss:  1.39e-11\n",
      "Extrapolation Loss: 5.26e+11 7.83e+13 1.09e+32 1.96e+34 6.34e+106 1.15e+105 \n",
      "Interpolation Loss:  5.85e-12\n",
      "Extrapolation Loss: 3.29e-04 2.35e-03 1.70e+03 1.41e+03 1.38e+29 1.26e+29 \n",
      "Interpolation Loss:  1.43e-11\n",
      "Extrapolation Loss: 8.63e+12 2.85e+10 2.11e+33 1.12e+31 8.16e+100 5.56e+100 \n",
      "Interpolation Loss:  8.74e-12\n",
      "Extrapolation Loss: 2.59e-01 1.64e+00 6.23e+08 2.06e+08 3.20e+42 2.08e+42 \n",
      "Interpolation Loss:  2.18e-11\n",
      "Extrapolation Loss: 5.95e+32 2.50e+32 4.39e+69 2.71e+69 1.95e+196 4.45e+200 \n",
      "Interpolation Loss:  1.78e-11\n",
      "Extrapolation Loss: 1.82e+20 1.49e+19 2.94e+47 1.41e+46 1.70e+139 1.70e+138 \n",
      "Interpolation Loss:  1.36e-11\n",
      "Extrapolation Loss: 1.05e+14 4.51e+13 2.24e+41 2.57e+35 1.29e+112 7.93e+113 \n",
      "\n",
      "\n",
      "Train range: [0,10]\n",
      "Interpolation Loss:  1.36e+02\n",
      "Extrapolation Loss: 3.35e+05 1.72e+08 3.02e+07 4.31e+06 2.67e+04 9.30e+06 \n",
      "Interpolation Loss:  1.60e+01\n",
      "Extrapolation Loss: 7.01e+04 6.68e+04 1.35e+06 1.59e+06 2.66e+10 2.49e+10 \n",
      "Interpolation Loss:  2.03e+02\n",
      "Extrapolation Loss: 1.14e+08 6.08e+10 1.04e+10 1.04e+10 5.39e+16 5.69e+16 \n",
      "Interpolation Loss:  2.13e+01\n",
      "Extrapolation Loss: 2.07e+06 2.09e+06 1.03e+08 9.92e+07 3.24e+13 3.05e+13 \n",
      "Interpolation Loss:  4.73e+02\n",
      "Extrapolation Loss: 1.49e+08 1.24e+09 5.29e+07 2.07e+09 2.68e+09 1.40e+07 \n",
      "Interpolation Loss:  5.97e+01\n",
      "Extrapolation Loss: 1.33e+04 4.23e+05 1.25e+04 1.85e+03 2.89e+09 4.99e+09 \n",
      "Interpolation Loss:  1.48e+03\n",
      "Extrapolation Loss: 2.29e+09 1.43e+11 7.90e+13 4.11e+11 1.62e+17 8.15e+17 \n",
      "Interpolation Loss:  1.37e+02\n",
      "Extrapolation Loss: 5.09e+05 1.18e+06 1.53e+07 6.35e+08 1.36e+12 1.35e+12 \n",
      "Interpolation Loss:  7.98e+02\n",
      "Extrapolation Loss: 8.30e+08 2.32e+06 3.94e+05 1.43e+08 7.01e+06 5.39e+06 \n",
      "Interpolation Loss:  1.08e+02\n",
      "Extrapolation Loss: 5.19e+06 5.74e+06 3.55e+08 2.92e+08 1.50e+14 1.48e+14 \n",
      "\n",
      "\n",
      "Train range: [-10,10]\n",
      "Interpolation Loss:  2.28e-12\n",
      "Extrapolation Loss: 4.38e-09 4.71e-09 4.49e-07 4.61e-07 3.96e+00 5.04e+00 \n",
      "Interpolation Loss:  2.22e-12\n",
      "Extrapolation Loss: 7.12e-08 7.47e-08 3.06e-05 3.53e-05 3.01e+04 4.68e+04 \n",
      "Interpolation Loss:  8.83e-13\n",
      "Extrapolation Loss: 3.06e+01 2.25e+02 4.40e+07 6.02e+07 7.76e+28 6.88e+28 \n",
      "Interpolation Loss:  1.99e-12\n",
      "Extrapolation Loss: 3.10e-06 2.07e-06 3.36e-03 3.86e-03 4.40e+08 4.88e+08 \n",
      "Interpolation Loss:  4.02e-12\n",
      "Extrapolation Loss: 1.78e-09 1.01e-10 3.66e-09 1.91e-10 3.59e-08 1.91e-09 \n",
      "Interpolation Loss:  5.72e-13\n",
      "Extrapolation Loss: 2.19e-06 1.39e-06 4.93e-03 4.90e-03 2.69e+08 5.13e+08 \n",
      "Interpolation Loss:  2.00e-12\n",
      "Extrapolation Loss: 5.40e-03 2.67e-03 3.60e+01 4.43e+01 3.99e+16 1.24e+16 \n",
      "Interpolation Loss:  1.60e+00\n",
      "Extrapolation Loss: 2.42e+02 2.62e+01 4.74e+02 3.60e+01 4.73e+03 4.91e+02 \n",
      "Interpolation Loss:  4.26e-12\n",
      "Extrapolation Loss: 2.34e-09 9.32e-11 4.66e-09 1.46e-10 4.75e-08 1.51e-09 \n",
      "Interpolation Loss:  1.20e-12\n",
      "Extrapolation Loss: 4.10e-08 9.04e-08 1.94e-05 3.14e-05 3.64e+04 4.86e+04 \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "save_all = False\n",
    "\n",
    "init = 'Kai_uni'\n",
    "model_param =  \"NALU\"\n",
    "\n",
    "test_per_range = 10\n",
    "sample_size = 100\n",
    "set_size = 200\n",
    "\n",
    "in_dim = sample_size\n",
    "hidden_dim = 1\n",
    "out_dim = 2\n",
    "num_layers = 1\n",
    "\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "batch_size = 1\n",
    "#values = [1] #np.linspace(1,1000,10)\n",
    "values = [[0,1],[-1,1],[0,10],[-10,10]]\n",
    "extr_scale = [5,10,100]\n",
    "extr_values = [[0,1],[-1,1]]\n",
    "\n",
    "datadict = {}\n",
    "for value in values:\n",
    "    datadict[str(value)] = []\n",
    "    for e_s in extr_scale:\n",
    "        for e_val in extr_values:\n",
    "            datadict[str(value)+str(e_s)+str(e_val)] = []\n",
    "\n",
    "\n",
    "for j, value in tqdm(enumerate(values)):\n",
    "\n",
    "    min_value = value[0]\n",
    "    max_value = value[1]\n",
    "\n",
    "    print(\"Train range: [\"+str(min_value)+\",\"+str(max_value)+\"]\")\n",
    "    for k in range(test_per_range):\n",
    "        #i=0\n",
    "        \n",
    "        model = NALU(num_layers, in_dim, hidden_dim, out_dim, init)\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=lr)\n",
    "\n",
    "        x_train, y_train, boundaries = generate_synthetic_selection_dataset(min_value, \n",
    "                                                                            max_value, sample_size, \n",
    "                                                                            set_size, boundaries = None)\n",
    "\n",
    "        x_test, y_test, _ = generate_synthetic_selection_dataset(min_value, max_value,\n",
    "                                                         sample_size, set_size, boundaries = boundaries)\n",
    "\n",
    "        x_train = x_train.type(torch.DoubleTensor)\n",
    "        y_train = y_train.type(torch.DoubleTensor)\n",
    "        x_test = x_test.type(torch.DoubleTensor)\n",
    "        y_test = y_test.type(torch.DoubleTensor)\n",
    "\n",
    "        loss = train(model, optimizer, x_train, y_train, epochs, batch_size, model_param)\n",
    "        out = loss.data.numpy() / np.max(x_test.data.numpy())\n",
    "        datadict[str(value)].append(out)\n",
    "        \n",
    "        print(\"Interpolation Loss: \",'{:.2e}'.format(out))\n",
    "        print(\"Extrapolation Loss: \", end='')\n",
    "        for e_s in extr_scale:\n",
    "            for e_val in extr_values:\n",
    "                x_test, y_test, _ = generate_synthetic_selection_dataset(e_s*e_val[0]*value[1], e_s*e_val[1]*value[1],\n",
    "                                                                 sample_size, set_size, boundaries = boundaries)\n",
    "\n",
    "                x_test = x_test.type(torch.DoubleTensor)\n",
    "                y_test = y_test.type(torch.DoubleTensor)               \n",
    "                test_loss  = test(model, x_test, y_test)\n",
    "                out = test_loss.data.numpy() / np.max(x_test.data.numpy())\n",
    "                datadict[str(value)+str(e_s)+str(e_val)].append(out)\n",
    "                print('{:.2e}'.format(out), end=' ')\n",
    "                #print(datadict)\n",
    "        print('')\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$9.60e+13$  & $4.76e+11$  & $3.63e+06$  & $1.13e-06$  & \n",
      "$1.25e+36$  & $1.11e+33$  & $4.16e+07$  & $1.69e-03$  & \n",
      "$2.31e+104$  & $2.18e+104$  & $6.91e+11$  & $3.33e+04$  & \n",
      "$3.00e+13$  & $2.04e+13$  & $4.03e+06$  & $7.38e-07$  & \n",
      "$6.87e+32$  & $9.83e+33$  & $2.18e+08$  & $1.95e-03$  & \n",
      "$1.81e+107$  & $5.76e+104$  & $6.86e+11$  & $4.77e+04$  & \n"
     ]
    }
   ],
   "source": [
    "for e_val in extr_values:\n",
    "    for e_s in extr_scale:\n",
    "        for value in values:\n",
    "            print('${:.2e}$'.format(np.median(datadict[str(value)+str(e_s)+str(e_val)])),' & ',end='')\n",
    "        print('')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$7.34e+23 }$ & $5.95e+31 }$ & $3.39e+08 }$ & $2.72e+01 }$ & \n",
      "$4.36e+62 }$ & $4.39e+68 }$ & $7.90e+12 }$ & $4.40e+06 }$ & \n",
      "$6.91e+170 }$ & $1.95e+195 }$ & $2.16e+16 }$ & $7.76e+27 }$ & \n",
      "$5.35e+27 }$ & $2.50e+31 }$ & $2.05e+10 }$ & $2.52e+01 }$ & \n",
      "$2.88e+59 }$ & $2.71e+68 }$ & $4.25e+10 }$ & $6.02e+06 }$ & \n",
      "$6.14e+172 }$ & $4.45e+199 }$ & $8.72e+16 }$ & $6.88e+27 }$ & \n"
     ]
    }
   ],
   "source": [
    "for e_val in extr_values:\n",
    "    for e_s in extr_scale:\n",
    "        for value in values:\n",
    "            print('${:.2e}'.format(np.mean(datadict[str(value)+str(e_s)+str(e_val)])),'}$ & ',end='')\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
