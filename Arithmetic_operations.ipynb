{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.nalu import NALU\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_arithmetic_dataset(arithmetic_op, min_value, max_value, sample_size, set_size, boundaries = None):\n",
    "    \"\"\"\n",
    "    generates a dataset of integers for the synthetics arithmetic task\n",
    "\n",
    "    :param arithmetic_op: the type of operation to perform on the sum of the two sub sections can be either :\n",
    "    [\"add\" , \"subtract\", \"multiply\", \"divide\", \"root\", \"square\"]\n",
    "    :param min_value: the minimum possible value of the generated integers\n",
    "    :param max_value: the maximum possible value of the generated integers\n",
    "    :param sample_size: the number of integers per sample\n",
    "    :param set_size: the number of samples in the dataset\n",
    "    :param boundaries: [Optional] an iterable of 4 integer indices in the following format :\n",
    "    [start of 1st section, end of 1st section, start of 2nd section, end of 2nd section]\n",
    "    if None, the boundaries are randomly generated.\n",
    "    :return: the training dataset input, the training true outputs, the boundaries of the sub sections used\n",
    "    \"\"\"\n",
    "    scaled_input_values = np.random.uniform(min_value, max_value, (set_size, sample_size))\n",
    "\n",
    "    if boundaries is None:\n",
    "        boundaries = [np.random.randint(sample_size) for i in range(4)]\n",
    "        boundaries[1] = np.random.randint(boundaries[0], sample_size)\n",
    "        boundaries[3] = np.random.randint(boundaries[2], sample_size)\n",
    "    else:\n",
    "        if len(boundaries) != 4:\n",
    "            raise ValueError(\"boundaries is expected to be a list of 4 elements but found {}\".format(len(boundaries)))\n",
    "\n",
    "    a = np.array([np.sum(sample[boundaries[0]:boundaries[1]]) for sample in scaled_input_values])\n",
    "    b = np.array([np.sum(sample[boundaries[2]:boundaries[3]]) for sample in scaled_input_values])\n",
    "\n",
    "    true_outputs = None\n",
    "    if \"add\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a + b\n",
    "    elif \"sub\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a - b\n",
    "    elif \"mult\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a * b\n",
    "    elif \"div\" in str.lower(arithmetic_op):\n",
    "        true_outputs = a / b\n",
    "    elif \"square\" == str.lower(arithmetic_op):\n",
    "        true_outputs = a * a\n",
    "    elif \"root\" in str.lower(arithmetic_op):\n",
    "        true_outputs = np.sqrt(a)\n",
    "    \n",
    "    scaled_input_values = torch.tensor(scaled_input_values, dtype=torch.float32)\n",
    "    true_outputs = torch.tensor(true_outputs, dtype=torch.float32).unsqueeze(1)\n",
    "        \n",
    "    return scaled_input_values, true_outputs, boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.nalu import NALU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_NUM = 100\n",
    "epochs = 100000\n",
    "batch_size = 1\n",
    "operator = \"add\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NALU(\n",
      "  (model): Sequential(\n",
      "    (0): NeuralArithmeticLogicUnitCell(\n",
      "      in_dim=100, out_dim=2\n",
      "      (nac): NeuralAccumulatorCell(in_dim=100, out_dim=2)\n",
      "    )\n",
      "    (1): NeuralArithmeticLogicUnitCell(\n",
      "      in_dim=2, out_dim=1\n",
      "      (nac): NeuralAccumulatorCell(in_dim=2, out_dim=1)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "in_dim = FEATURES_NUM\n",
    "hidden_dim = 2\n",
    "out_dim = 1\n",
    "num_layers = 2\n",
    "\n",
    "dim = in_dim # dimensition for generating data\n",
    "\n",
    "model = NALU(num_layers, in_dim, hidden_dim, out_dim)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch \t 0 \t loss \t 1514.3577 \t acc \t 0.0\n",
      "epoch \t 1000 \t loss \t 156.40047 \t acc \t 0.0\n",
      "epoch \t 2000 \t loss \t 79.59128 \t acc \t 0.0\n",
      "epoch \t 3000 \t loss \t 69.66294 \t acc \t 0.0\n",
      "epoch \t 4000 \t loss \t 68.88185 \t acc \t 0.0\n",
      "epoch \t 5000 \t loss \t 68.53078 \t acc \t 0.0\n",
      "epoch \t 6000 \t loss \t 68.276215 \t acc \t 0.0\n",
      "epoch \t 7000 \t loss \t 68.18274 \t acc \t 0.0\n",
      "epoch \t 8000 \t loss \t 68.146835 \t acc \t 0.0\n",
      "epoch \t 9000 \t loss \t 68.21443 \t acc \t 0.0\n",
      "epoch \t 10000 \t loss \t 68.34964 \t acc \t 0.0\n",
      "epoch \t 11000 \t loss \t 68.39111 \t acc \t 0.0\n",
      "epoch \t 12000 \t loss \t 68.34775 \t acc \t 0.0\n",
      "epoch \t 13000 \t loss \t 68.31914 \t acc \t 0.0\n",
      "epoch \t 14000 \t loss \t 68.29676 \t acc \t 0.0\n",
      "epoch \t 15000 \t loss \t 68.27421 \t acc \t 0.0\n",
      "epoch \t 16000 \t loss \t 68.24491 \t acc \t 0.0\n",
      "epoch \t 17000 \t loss \t 68.19986 \t acc \t 0.0\n",
      "epoch \t 18000 \t loss \t 68.13751 \t acc \t 0.0\n",
      "epoch \t 19000 \t loss \t 68.06501 \t acc \t 0.0\n",
      "epoch \t 20000 \t loss \t 67.990906 \t acc \t 0.0\n",
      "epoch \t 21000 \t loss \t 67.92348 \t acc \t 0.0\n",
      "epoch \t 22000 \t loss \t 67.86417 \t acc \t 0.0\n",
      "epoch \t 23000 \t loss \t 67.70433 \t acc \t 0.0\n",
      "epoch \t 24000 \t loss \t 67.855896 \t acc \t 0.0\n",
      "epoch \t 25000 \t loss \t 67.69874 \t acc \t 0.0\n",
      "epoch \t 26000 \t loss \t 67.78543 \t acc \t 0.0\n",
      "epoch \t 27000 \t loss \t 67.79641 \t acc \t 0.0\n",
      "epoch \t 28000 \t loss \t 67.73627 \t acc \t 0.0\n",
      "epoch \t 29000 \t loss \t 67.77867 \t acc \t 0.0\n",
      "epoch \t 30000 \t loss \t 67.69223 \t acc \t 0.0\n",
      "epoch \t 31000 \t loss \t 67.75562 \t acc \t 0.0\n",
      "epoch \t 32000 \t loss \t 67.808105 \t acc \t 0.0\n",
      "epoch \t 33000 \t loss \t 67.66221 \t acc \t 0.0\n",
      "epoch \t 34000 \t loss \t 67.573524 \t acc \t 0.0\n",
      "epoch \t 35000 \t loss \t 67.73226 \t acc \t 0.0\n",
      "epoch \t 36000 \t loss \t 67.780365 \t acc \t 0.0\n",
      "epoch \t 37000 \t loss \t 67.72568 \t acc \t 0.0\n",
      "epoch \t 38000 \t loss \t 67.690796 \t acc \t 0.0\n",
      "epoch \t 39000 \t loss \t 67.705696 \t acc \t 0.0\n",
      "epoch \t 40000 \t loss \t 67.7444 \t acc \t 0.0\n",
      "epoch \t 41000 \t loss \t 67.6835 \t acc \t 0.0\n",
      "epoch \t 42000 \t loss \t 67.70915 \t acc \t 0.0\n",
      "epoch \t 43000 \t loss \t 67.711914 \t acc \t 0.0\n",
      "epoch \t 44000 \t loss \t 67.77633 \t acc \t 0.0\n",
      "epoch \t 45000 \t loss \t 67.76993 \t acc \t 0.0\n",
      "epoch \t 46000 \t loss \t 67.769104 \t acc \t 0.0\n",
      "epoch \t 47000 \t loss \t 67.74443 \t acc \t 0.0\n",
      "epoch \t 48000 \t loss \t 67.728645 \t acc \t 0.0\n",
      "epoch \t 49000 \t loss \t 67.54111 \t acc \t 0.0\n",
      "epoch \t 50000 \t loss \t 67.732025 \t acc \t 0.0\n",
      "epoch \t 51000 \t loss \t 67.69348 \t acc \t 0.0\n",
      "epoch \t 52000 \t loss \t 67.60472 \t acc \t 0.0\n",
      "epoch \t 53000 \t loss \t 67.72211 \t acc \t 0.0\n",
      "epoch \t 54000 \t loss \t 67.58179 \t acc \t 0.0\n",
      "epoch \t 55000 \t loss \t 67.53382 \t acc \t 0.0\n",
      "epoch \t 56000 \t loss \t 67.53006 \t acc \t 0.0\n",
      "epoch \t 57000 \t loss \t 67.655304 \t acc \t 0.0\n",
      "epoch \t 58000 \t loss \t 67.52811 \t acc \t 0.0\n",
      "epoch \t 59000 \t loss \t 67.929184 \t acc \t 0.0\n",
      "epoch \t 60000 \t loss \t 67.542404 \t acc \t 0.0\n",
      "epoch \t 61000 \t loss \t 67.6051 \t acc \t 0.0\n",
      "epoch \t 62000 \t loss \t 67.64814 \t acc \t 0.0\n",
      "epoch \t 63000 \t loss \t 67.71512 \t acc \t 0.0\n",
      "epoch \t 64000 \t loss \t 67.74194 \t acc \t 0.0\n",
      "epoch \t 65000 \t loss \t 67.77723 \t acc \t 0.0\n",
      "epoch \t 66000 \t loss \t 67.61856 \t acc \t 0.0\n",
      "epoch \t 67000 \t loss \t 67.73799 \t acc \t 0.0\n",
      "epoch \t 68000 \t loss \t 67.71686 \t acc \t 0.0\n",
      "epoch \t 69000 \t loss \t 67.72068 \t acc \t 0.0\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, boundaries = generate_synthetic_arithmetic_dataset(operator, -10, 10, FEATURES_NUM, 1000)\n",
    "X_test, y_test, _ = generate_synthetic_arithmetic_dataset(operator, -100, 100, FEATURES_NUM, 1000, boundaries)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),lr=0.01)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch in range(len(X_train) // batch_size):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        X_batch_train = X_train[batch:(batch+batch_size),:]\n",
    "        y_batch_train = y_train[batch:(batch+batch_size),:]\n",
    "    \n",
    "        out = model(X_batch_train)\n",
    "\n",
    "        loss = F.mse_loss(out, y_batch_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    \n",
    "    if epoch % 1000 == 0:\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        X_batch_test = X_test[batch:(batch+batch_size),:]\n",
    "        y_batch_test = y_test[batch:(batch+batch_size),:]\n",
    "        \n",
    "        output_test = model(X_batch_train)\n",
    "        \n",
    "        acc = np.sum(np.isclose(output_test.detach().numpy(), y_batch_test.detach().numpy(), atol=.1, rtol=0)) / len(y_batch_test)\n",
    "        print(\"epoch \\t\", epoch, \"\\t loss \\t\",  loss.detach().numpy(), \"\\t acc \\t\", acc)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
